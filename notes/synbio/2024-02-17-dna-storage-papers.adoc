---
title: "DNA Storage Notes"
date: 2024-02-17
tags: synbio, paper
---

:toc:

== Robust Chemical Preservation of Digital Information on DNA in Silica with Error-Correcting Codes


== HEDGES error-correcting code for DNA storage corrects indels and allows sequence constraints

=== Significance
This paper constructs an error-correcting code for the {A, C,
G, T} alphabet of DNA. By contrast with previous work, the
code corrects insertions and deletions directly, in a single
strand of DNA, without the need for multiple alignment of
strands. This code, when coupled to a standard outer code, can
achieve error-free storage of petabyte-scale data even when
∼10% of all nucleotides are erroneous.

* most errors are indels, over 50%: insertions/deletions
* most ECCs only correct substitutions, more common in tradditional information theory
* tradditional dna storage: sequencing to high depth, followed by multiple alignment and consensus base calling
** only deal with errors associated with DNA sequencing?
** small code rates because of high rate of repetition

* inner code: translates between ATGC and 01
* outer code: corrects residual errors with high probability
** RS code applied diagonally
*** RS(255, 223)

=== results
* autokey cipher:
** pseuorandom characters "depends deterministically, via a hash function, on a fixed number of previous message bits {b_j}, on the current bit position index i of the current message bit b_i and the strand ID"
** then emit character C_i = K_i + b_i
*** redundancy: at each i, C_i can take on only two out of four values, because b = {0, 1}
** the output DNA is pseudorandom.

message bits
* decode: sequentially gusses message bits
** if guess agrees, score increases, otherwise score is penalized as substitution, if the guess would agree at another position, insertion or deletion is assumed to occur
** keeps track via a heap, prevents exponential growth using variant of A^* algorithm

* any single decoding error in nucleotides will poison downstream predictions -> only one good-scoring chain of guesses

* unsuitable for very short DNA strands

code: "The computer code used for the generation and testing of the inner
HEDGES code and outer RS code is available at https://github.com/whpress/
hedges. This paper utilized two commercial C++ source code libraries:
Numerical Recipes (http://www.numerical.recipes) and the Schifra Reed-
Solomon Error Correcting Code Library (http://www.schifra.com). The spe-
cific routines used in this paper are freely available for noncommercial use and are included in the above GitHub repository."

== DNA Fountain enables a robust and efficient storage architecture
* upper bound in information storage: 2 bits/base
** biochemical constraints reduce to 1.98 bits/base
** overall shannon information density: 1.83 bits/base

* redundancy:
** dividing file into overlapping segments: poorly scalable and loss of information, still had small gaps in retrieved information
** RS codes on small blocks of input data -> could perfectly retrieve information
*** blocked RS codes: large variations in dropout rate within protected block

=== DNA fountain
* fountain codes adapted to deal with oligo dropouts and biochemical constraints
* steps:
** binary file into series of nonoverlapping segments
** luby transform: package data into short messages (droplets) via special distribution and adding them bitwise under binary field
*** droplet: 38 bytes (2 bytes for RS, 4 for random number generator, 32 for data payload)
*** seed: allows decoder to infer indentities of segments
** screening: keep iterating over luby transform
*** dna sequence 152 nucleotides, 200 nt long with annealing sites for illumina adapters

** despite copies of the original DNA sequence being worse (via PCR), could still fully recover the file

** code: https://github.com/TeamErlich/dna-fountain

== Paper: The DNA Data Storage Model

https://www.computer.org/csdl/magazine/co/2023/07/10154188/1O1wUGjK41O[Source]

== points about dna
*  no preexisting media 
*  detached from array based structure (metadata encoded within each dna segment)
*  error characteristics

== overview 
. encoding
    *  turning binary into ternary into bases
    *  more error prone sequences should be avoided
. synthesis
. storage
. retrieval 
. sequencing
. decoding

image::/images/synbio/dnachannel.png[]

== application, presentation, session
*  done in binary 
*  application: how is this storage to be used? 
**  most likely archival 
*  presentation: preparation of bitstreams
*  session: access to the DNA storage interface
**  object based

== DNA channel
*  takes bitstream or digital object from session layer and processes those bits 
*  implemented as software codec
*  when bitstream is presented: 
**  packetization: longest strand of DNA that can be currently made (300 bases), so the bitsreams must be broken down
*** add indices to each segment
** error correction: add redundant information to reduce probability of random errors
*** augment segments (inner code) or add segments (outer code)
** translation: binary to bases 
** transformations: repeated bases (homopolymers), high proportion of G and C and some other patterns can cause errors

== DNA physical layer
*  synthesis (writing)
    - silcon technology based implementations that miniaturize synthesis device
*  storage 
    - store in dry and chemically inert environment
    - organized with physical tape system
*  reading (sequencing)
    - sequencing-by-synthesis: high accuracy, high latency
    - nanopore sequencing: lower latency,  lower accuracy
*  retrieval (important for DNA storage!)
    - easy: read entire archive
    - if multiple objects stored in archive, random access operations are required
* PCR and DNA pull-out with magngetic nanoparticles
** PCR:
*** session layer assign a set of object IDs, and at encoding time channal layer appends a set of predefined DNA base sequences to both ends of a DNA sequence belonging to an object (target sites)
*** probes (short DNA sequences): that complement target site attach to target molecule along with polymerases
*** now the primers enable polymerase to copy target DNA molecules, creating more DNA molecules of the object of interest than other DNA molecukes
*** result is pool of mostly DNA molecules belonging to object we want
** DNA pull-out  
*** channel layer only needs to append the object IDs as target sites at one end of every DNA sequence belonging to an object
*** probes attached to magnetic nanoparticles
*** probes bind to DNA molecules and can separate them

== problem for our team to investigate
*  make retrieval of information easier

== sources
. https://ieeexplore.ieee.org/document/10154188?denied=[The DNA Data Storage Model]

== Paper: Efficiently Enabling Block Semantics and Data Updates in DNA Storage

https://arxiv.org/abs/2212.13447[Source]

== abstract
* storage space into fixed-sized units
* pair of random access PCR primers of length 20 define an independent storage partition, which is managed indepedently of other partitions
* transform internal addressing scheme of a partition into an equivalent PCR-compatible -> run PCR with primers that can be variably elongated to include desired part of internal address
** retrieve a specific block with high accuracy
** 140x reduction in sequencing cost and latency

== introduction
* similarity search
* runtimes for automating wetlab protocols
* provide random access at nearly constant latency via PCR
* current state of the art: key-valye object stores
** pair of primers define kay and arbitrarly sized value stored in molecules tagged with same primers
* large objects stored across many molecules, all of which are retrieved with sufficient uniformity
** internal address: part of molecule that uniquely identify and re-establish order
** object than spans N molecules requires log_4 N bases for indexing
* primers:
** lots of unique combinations but all aren't appropriate
** must be balanced: GC content (50%)
** all primers used in same DNA sample must be significantly different from each other in Hamming distance to avoid amplication of unwanted data
*** minimum pairwise distance (a problem) only around 1000-3000 primers can fit the constraints
* can only chemically distinguish 1000 different objects
* with 1TB of data, the smallest unit of access is 1GB, so we always have to sequence this much DNA, no matter how small the unit we want is
* using longer primers doesn't work: compatible primers scale linearly
* can't build a functional data storage because of:
** allowing arbitrary size, can't easily get small objects
*** high Hamming distance
*** many cycles of PCR
*** object-based design results in flat architechture
**** no logical order/distance metric between objects
*** limiting sequential access to a single object -> random access

paper's proposal: block storage semantics
- each block independently read/written to
- group of consecutive blocks can be efficieetnly retriveed
- numer of mutually compatible primer pairs is limited, internal address space avalible to any pair of primers is virutally unlimited

contributions
- flexibility and constraints of internal address space for pair of primers -> implement block storage sementics
- PCR-compatible indexes to enable random access with primers than can be enlongated
- organize partitions similar to version control
- wetlab evaluation with state-of-the-art DNA storage architecture

== background

=== 2.1 dna storage basics
- dna molecule 300 bases long can store 75 bytes
- have to break down files into shorter molecules, and add an internal address or index

==== 2.1.1 encoding
* using a coding scheme (constrained coding) that
** prevent occurrence of homopolyers
** balance GC content
* unconstrained coding: homopolymers occur with low probability
** significantly increases coding density with practical ranges of error rates
** used in this paper
** handle error types with outer Reed-Solomon ECC codes
* to encode internal address within every molecule
** use more complex constrained coding scheme

image::/images/block/fig1.png[]

* once file split into pieces and encoded into DNA, pair of primers added to end and beginning of each string
** chemical tag that logically group related molecules together, allowing for random access

==== 2.1.2 data retrieval
* PCR used to selectively amplify molecules containing file peices
** molecules isolated using primers
** sequencing produces many DNA strings: reads
** errors are manifested in reads
** sequencing coverage/depth: the higher the coverage, the easier to reconstruct from these reads, but higher cost
* obtained reads clustered based on similarity (should be from same molecule) via Levenshtein distance
* consensus finding algorithm to extract original DNA string

==== 2.1.3 decoding and error correction
* using internal address, binary data used to recreate original file
* errors corrected using Reed-Solomon or LDPC codes
* state of the art architecture treats all DNA molecules as columns in matrix
** seperate DNA molecules created as external ECC code, one codeword rerepsent a row -> high information density it creates strong inter-molecular dependencies

==== 2.1.4 PCR
* doubles number of DNA molecules in every cycle
* three phases: denaturation, annealing, extension
* optimal primer length: 20
* can go to 40 and max length 100
* internal addresses can be very similar, not balanced GC content and repeated base

=== existing data update mechanisms
* direct edits of DNA molecules: limited to updates to single molecule, can only be applied when size of data does not change
* current state of the art orgzanizations have data split across many molecules, significant intermolecular data dependencies
* instead of storing data as nucleotides, store as nicks: rewritable
** but cannot do PCR, sacrificing random access
** storage density is 50-fold lower

== 3 managing internal address space
* pair of primers of length 20
* length of DNA strands: 150
* maximum storage capacity achieved when entire portion of NA used for indexing (no actual data stored)
* density highest when onle one molecule, no index needed (only for tiny objects)
* longer primers reduct information density significantly, loss diminishes linearly with longer strand length

== 3.1 parition architecture
* address space AAA...AAA to TTT...TTT with index length L represented as prefix tree
** index of length L covers addresses from AAA...AAA to TTT...TTT: 1D array of 4^L fixed capacity storage units (payload of one molecule)
**  any contigous byte-range can be statically mapped to contigous index-range and vice versa
* contiguous index-range be can precisely described with few prefixes: AAA to AGT -> AA, AC, AG and AT
* leaf is DNA strand
* maximum information density achieved with all DNA strands have same index length and all indexes present
* any contiguous range of bytes within parition could be retrived quite precisely with single PCR if primers were extended to include part of the index
* future work: to improve efficiency of sequential accesses: set of files could be mapped onto the partition in a manner that tries to optimally align the files to nodes in prefix tree

== 3.2 concentration constraints
* to manage cost of sequencing: every strand in DNA storage should be represented in equal concentrations
** highly concentrated strands will be sequenced at higher coverage while other strands need more sequencing to be represented, wasting resources
** for random black access to work: desired sequences need to be amplified much higher because indexes are similar
** PCR may overwrite their index to desired index, resulting in exponential amplification -> mispriming
*** multiple dna strands, can't use consensus to select which one
** to ensure dominance of desired strands: ensure closet targets in pool are not present in higher concentration
* all nodes at same level of index tree map to similar number of DNA strands and in similar concentrations
** make sure data updates do not compromise this balance

== 4 random block access

== 4.1 general approach
* indexes AAA...AAA to TTT...TTT are not PCR-compatible

image::/images/block/primer.png[]

* prior work: follow maximum information density design -> no control over their structure
* use less dense encoding of indexes, minor loss in information density
* added sparsity provide protection against errors in index

== 4.2 requirements for elongated primers
* every pair of main primer: high mutual distance so can extract target partition regardless of its size and concentration relative to other paritions
** two similar primers: P_a and P_b
*** target data is dominant because of uniform concentration of data within same level of index hierarchy
*** ensuring high distance between indexes, is not as important as it is for main primers
* GC content: primers have slightly more restrictions
** primers are not fixed length: add sparsity to indexes so GC content is uniform with every elongation

== 4.3 PCR-Navigable Index Tree
* prefix tree is randomized (the order of edges): to prevent incomplete, unbalanced, degenerate trees
* sparse out addresses: add extra letter between every two adjacent edges
** perfectly balance the GC content (maximize Hamming distance between sibling nodes)
*** guatantees near perfect GC content and disable indexes of more than 2
* 10-based long internal address: 3% information density loss
** added flexibility reduces sequencing costs

== 4.4 Index Tree Management
* different seeds for different partitions to ensure vastly different trees to avoid unwanted molecular interactions

== 5 Data Updates in DNA Storage
* low-latency enzymatic synthesis

== 5.1 naive
* throw away old data, update software to just use new primers
* problems:
** recreate parition (expensive, arbitrary amount of information)
** wasting primres

== 5.2 versioning
* updates logged as ordered series of incremental patches
* minimal set of DNA molecules at same concentration
* application of updates done in software, not in-situ chemically
* how should updates be tagged and their primers?
* how to retrieve updated DNA and how to mix original DNA with updated DNA

== 5.3 placement of updates in address space
* updates in their own partition with dedicated set of primers
* have to potentially read all updates, none of which apply to data (becaues data might have been updated)
* updates embedded into address space of each primer pair -> reading updated data requires single PCR
** but have to read all data under same primer pair -> lots of data
* final: data and updates interwoven
** allocate blocks for updates, some unused, overflow data is given a pointer
*** object with prefix ACGT: original object at ACGTA, first update: ACGTC, second update ACGTG
**** link between data and updates, no bookkeeping
*** PCR uses primers with ACGT, software understands how to patch everything together

== 5.4 structure and semantics of updates
* ensure concentrations of original DNA and updates are similar as possible
* application of updates delegated to end-user or upper-application

== 5.5 physically mixing data and updates
* directly impacts cost of sequencing

== 6 methodology (skipped for now)

== 7 results

== 7.1 baseline random access
* sequencing of 99.66% unwanted data

== 7.2 random block access
* mispriming is an issue with similar primers
* most errors caused by blocks amplified through mispriming

== 7.3 sequencing cost reduction
* ability to retrieve individual data blocks
* sequencing cost is proportional to size of sequencing cost

== 7.4 sequencing latency reduction
* ability to select given block, but latency depends on partition size and sequencing technology
** block based: retrivign single block reduces number of runs needed
* NGS: sequencing output only avalible at end of run
* nanopore: output size dependent, continuosly produced and analyzed in real time
* post-sequencing/data movement and software decoding time also reduced (not a bottleneck so doesn't really matter)

== 7.5 cost of creating and retrieving updates
* naive system: new updated copy of entire partition and assigns new primer
* our system: precise access retrieves data and updates for specific block

== 7.6 mixing data and updates
* possible to keep concentrations of data and updates similar

== 7.7 scalabilty and limitations !!
=== 7.7.1 block count
* extend both primers instead of one
** two-sided extension by 10 characters -> 1024^2 addressable blocks, same order of magnitude as modern SSDs

=== 7.7.2 block size
* mispriming depends on number of blocks and structure and sparsity of their indexes
* no limits on block size

=== 7.73 partition count
* prevent mispriming: two PCR cycles
** first main primers
** second elongated primers
* zipfan distrbution: many blocks never accessed, few accessed very frequently

== 8 decoding procedure
* extract substring between elongated forward primer and reverse primer (payload)
* cluster payloads as per Rashtchian
* in descending order of cluster size: trace reconstruction using double sided BMA algorithm

== 8.1 handing of mispriming during pcr
* incorrectly amplified strands: indexes that were very close to indexes of target block in edit distance (2-3), rather than hamming
* to an extent, can be corrected through error correction codes

== 9 related work
* nested PCRs
** don't support multiplex-PCR (this does)
* nested PCR is only two levels deep, this is 6 levels deep

== conclusion
* slow storage media (DNA) so should do update applications in software


== Paper: Spatially Selective Electrochemical Cleavage of a Polymerase-Nucleotide Conjugate
** abstract
- enzymatic methods: functional, enzymatic, environmental advantages over phosphoramidite synthesis
    - must be parallelized -> polymerase-nucleotide conjugate cleaved using electrochemical oxidation on microelectrode array
    - developed conjugate maintains polymerase activity toward surface bound substrates with single base control
    - at scale

** intro
- phosphoramidite synthesis: high-throughput
  - expensive, environmentally hazardous organic solvents
- TdT: template-independent polymerase used for extension of growing ssDNA with dNTP
- deblocking: via chemical or photochemical means
  - this paper: electrochemical
- Jung: spatially selective deblocking of 3'aminoxy-protested oligonucleotide with nitrous acid produced along electrochemically generated pH gradient -> lay out pathway to paralization
- extension step: single step dNTP incorporation yeilds with TdT mutants are below those for phosphoramidite synthesis
- electrochemical deblocking step: regulate local nitrosamine concentrations and exposure times to avoid base-substitution in synthesized oligonucleotides
  - need additional engineering of mutant TdT polymerase and develoment of deprotection system


== notes for pitches

Cloud storage is ubiquitous. In addition to cloud storage data centers operated by technology companies and data-critical (healthcare, finance, and government) organizations, these entities must deal with a growing reliance on storing a lot of information that is rarely changed but required to be stored for long periods of time. This is referred to as an information explosion. We are predicted to generate around 180 zettabytes of information, with 22 zettabytes of data being shipped around, 60% in the form of hard drives.

What is long term storage? Long term storage is defined as storage that can persist information without the requirement for constant electricity. Silicon based long term storage includes: external hard drives, flash drives, CDs which last around 5-20 years. Silicon based storage works broadly like this: file is turned into a stream of bits of 0s or 1s, and these bits are stored as charges based on the binary encoding, then after these bits have been encoded, the storage media is stored in conditions that try to preserve this charge. Ubiquitous storage in the form of "the cloud" is becoming an everyday norm because most people do not wanna lug around pieces of storage. But cloud storage isn't really in the cloud. Cloud storage is stored in giant buildings full of hard drives, some buildings taking up "nearly 200 acres of land apiece". In addition, because Google and Microsoft promise that your data is preserved, multiple copies of your data is written to different data centers. Multiply that process a few times, and you can see that cloud data centers get hot, which requires lots of cooling, resulting in data centers using as much energy as an airline. It's not that the actual storage that requires so much cooling, but the computers that write all that data to disk which must be cooled. While these data centers account for around 2% of global energy usage, and because information is growing rapidly, these data centers will continue to eat up more land and energy. Generally speaking, data centers have reached their peak efficiency with the level of scalability require, which we can implicitly observe from Moore's Law.

DNA storage is an emerging technology for long term storage. Generally the steps of storing information into DNA goes as follows: are encoding, synthesis, storage, retrieval, and decoding. Compared to traditional long term storage mediums, DNA is more:

1. durable: if stored in optimal conditions and dehydrated, DNA can possibly endure for millions of years.
2. dense: up to 10^{19} bits per cubic centimeter, which is eight times more dense than other storage media.
3. energy efficient: due to the failure rate of current long term storage media, such as charges flipping, I predict less energy will be needed to store information in DNA molecules because we can store less copies of data given DNA's density. In fact, there are estimates that "it could reduce the energy consumption by 1,000 times compared to today's data centres"footnote:[https://www.nature.com/articles/d42473-022-00319-3].

Currently DNA storage the most durable and dense storage medium we have. However, it is not a easily scalable technology, which is what I believe synthetic biology can help with. Current industry standards of DNA synthesis is done through chemical means, phosphoramidite synthesis. Phosphoramidite synthesis is expensive and requires the use of toxic organic solvents. With sequencing, because DNA is so dense, if implemented with a naive storage model, it's easy for too much data to be sequenced because we may have to sequence an entire DNa molecule to get at a few bases in the middle of the molecule. This results in the output from sequencing to be wasted. And lastly, because mutating DNA in the wet lab is prone to mistakes and slower than traditional long term storage, this is another roadblock in realizing DNA as a long term storage medium.

Thus, because synthesis and sequencing remain as bottlenecks for using DNA as storage, our solution is to design a scalable DNA storage process with improved synthesis and storage.

An emerging method of DNA synthesis is enzymatic DNA synthesis with the use of TdT (template-independent polymerase). Aachen 2020 successfully demonstrated the use of TdT to synthesize DNA. We can go further to parallelize this process with an microelectrode array; this will be driven by wet lab.

To improve storage, our dry lab will implement block based semantics with elongated primers and an update system with "patches". Dry lab will also either write software or use open source code to encode and decode information into bases and deal with error correction, based on time constraints.

Why enzymatic DNA synthesis?
* terminal deoxynucleotidyl transferase (TdT) is the enzyme behind enzymatic DNA synthesis:
  . does not require preexisting strand of DNA
  . Uses soft chemicals
  . Provides kinetic control with enzymes
  . produce longer strands than with chemical synthesisfootnote:[https://www.forbes.com/sites/johncumbers/2022/03/25/dna-synthesis-goes-green/?sh=49f785fb532c]

* of course some downsides include that different bases add differently, which is an issue Aachen tackled.

Why block based semantics and updates via patches?

A flat key-value object based architecture forces the user to sequence an entire unit of random access, whether they want all that information or not. This results in a lot of waste because DNA is very dense.

With block based semantics:

* we design primers designed with main prefix and extended bases representing blocks:
** retrieve a block in a partition with higher accuracy
** blocks can be smaller than the unit of random access: sequencing smaller blocks of data
** reductions in sequencing cost and latency
** ability for sequential access by elongating a primer, similarly done in array based storage (tradditional storage mediums)
** primers based on tree based structure

* versioning: only write updates as new DNA molecule and tag with appropriate primer
** updates applied in software is: faster, more reliable

* based on Aachen's work, we can first set up the enzymatic DNA synthesis process as described by Church's paper.
* then to demonstrate improvement, we work on creating the novel, cleavable polymerase-nucleotide conjugate which allows for attachment to the microelectrode array, while maintaining the activity of the TdT enzyme.
** this allows for electrochemical cleavage/deblocking.
** compared to parallelized methods that rely on kinetic competitions, polymerase-nucleotide conjugate allows for single-base additions and the ability to further extend the oligonucleotide.
** electrochemical control vs. nitrous acid-mediated deblocking also means we don't rely on environmentally hazardous chemicals

We can employ the engineering cycle with our software algorithms, which we can referred to as the software codec. 

There needs to be a process for: 
1. Encoding
* requires generating primers to implement the block based semantics (via Hamming/Edit distance constraints and chemistry constraints like GC content) and splitting a file across several DNA molecules if necessary

2. Decoding
* error correction, reconstructing the file and applying update patches

3. Updating data
* because we aren't applying updates chemically, instead reading a DNA molecule gives us the original information and updates, which with long term storage shouldn't be alot. Using algorithms we can apply these updates to produce the final version of the data with updates applied.

For Modelling, we can model:

* the TdT enzyme using Michals-Menten kinetics
* how likely a primer is to "misprime" meaning the wrong primer is amplified, with protein modelling.
* optimal concentrations of DNA molecules are required to keep sequencing costs low and 
** this requires a feedback loop of wet lab giving us data from "Amplify-then-Measure" then dry lab using these numbers to estimate the costs of sequencing based on concentrations
* optimal conditions of added different bases

This project is very much interdisciplinary, so we must reach to many experts. Our wet lab portion is also chemistry heavy, so I have listed to chemistry professors who I believe will be very helpful.

* Specialists:
** Electrochemistry: Dr. Eva Nichols (UBC)
** Synthetic DNA: Dr. David Perrin (UBC)
** Data management: Dr. Richard Arias-Hernández (UBC)
** Bioinformatics (Error correction): Dr. Jiarui Ding (UBC)
* Outreach
** Education:
*** Computation and Information
*** DNA Storage
* BC Government, Hospitals
* Cloud storage companies

=== Current Long Term Storage
* https://en.wikipedia.org/wiki/Zettabyte_Era
* https://mit-serc.pubpub.org/pub/the-cloud-is-material/release/1
* https://contenteratechspace.com/6-best-ways-for-long-term-data-storage/
* https://medium.com/stanford-magazine/carbon-and-the-cloud-d6f481b79dfe
* https://www.cbc.ca/radio/spark/digital-data-has-an-environmental-cost-calling-it-the-cloud-conceals-that-researcher-says-1.6641268
* https://thereader.mitpress.mit.edu/the-staggering-ecological-impacts-of-computation-and-the-cloud/

=== DNA storage
* https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10296570
* https://www.computer.org/csdl/magazine/co/2023/07/10154188/1O1wUGjK41O
* https://www.nature.com/articles/d42473-022-00319


=== Wet lab
* https://www.nature.com/articles/s41467-019-10258-1[Terminator-free template-independent enzymatic DNA synthesis for digital information storage]
* https://2021.igem.org/Team:Aachen
* https://pubs.acs.org/doi/10.1021/acssynbio.3c00044[Spatially Selective Electrochemical Cleavage of a Polymerase-Nucleotide Conjugate]
* https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8288405/

* this approach is chemistry heavy, so based on the skills of our wet lab, we can also engineer yeast cells to produce DNA and store data in artifical chromosomes, as demonstrated by Yuanfootnote:[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8288405/]

=== Dry Lab
* https://www.nature.com/articles/srep14138
* https://arxiv.org/abs/2212.13447

