---
title: "Computational/Quantum Chemistry Papers"
date: 2024-07-04
---

== How Long Can A C−C σ‑Single Bond Be?

== AdsorbML: a leap in efficiency for adsorption energy calculations using generalizable machine learning potentials

- importance: adsorption energy of adsorbate and a catalyst surface of interest
- usually relies on heuristics and researcher intuition
- this paper: machine learning potentials can be leveraged too
- adsorption energy: energy associated with molecule interacting with catalyst surface
  - find adsorbate-surface configuration that minimizes structure's overall energy -> starting point of free energy diagrams
  - adsorption energies of intermediates are powerful descriptors -> correlate with experimental outcomes
- many ways for molecules to interact -> atom positions need to be relaxed until local energy minimum is reached -> most common method is DFT
  - first compute single-point calculation -> system's energy and per-atoms forces
  - relaxation perform local optimization -> per-atom forces are iteratively performed with DFT then used to update atom positions
  - goal find global minimum: minima hopping/sampling
- E_abs = E_sys - E_slab - E_gas
- constraints: adsorbate don't float away, adsorbate don't break, don't drastically change slab -> can lead to invalid or inaccurate adsorption energies
- heuristic for finding globally optmial minima -> surface symmetry
  - descriptor based vs graph based -> gets challenging with more degrees of freedom
  - DFT scales N^3 (N is number of electrons)
- ML and DFT combination?

- this paper: proposes hybrid approach: sample large number of potential adsorbate configurations (heuristic and random strategies)
  - perform relaxations using ML potentials
  - best k-relaxed energies refined using single-point DFT calculations/full DFT relaxations
- ML is only around 50% accurate when calculating relaxation energies

=== AdsorbML algorithm
- ML finds relaxation energies
- best k systems with lowest energies are selected
- DFT single-point dont on corresponding structures (more accurate) OR DFT relaxations performed form ML-relaxed structures (ML is pre-optimizer)
- ML is just used to rank the energies

=== Experiments
- experiments and different strategies -> most accurate moecuels do not necessarily find much better minima

=== Discussion
- pave the way for reducing computational cost in computational chemistry
- this AsdorbML allows us to choose compute budget
- how does noise impact ML models
- alternative methods of global optmization/initial configuration generation
  - hopping, constrained optimization, BO, directly learned appraoch
- ML relaxations without DFT are not accurate enough


== Automatic Differentiation for the Direct Minimization Approach to the Hartree−Fock Method
- automatic differentiation applied to HF method
  - calculate derivatives of function without giving explicit forms of derivatives
  - backward propagation algorithm -> special case of reverse-mode AD
  - reverse mode AD: more efficient than forward mode when num input variables > output variables
- reverse-mode more efficient than forward-mode, eigenvalue calculation in SCF method impedes use of reverse-mode AD (why)
- directly minimize HF energy under orthogonality constraint of MOs using reverse-mode AD by avoiding eigenvalue calculation
- HF: eigenvalue equation (Roothaan equation) via MOs as LCAOS, solved through SCF
  - requires calculating eigenvalues, and reverse-mode cannot be applied in degenerated systems
  - reverse mode is preferred because: multiple coefficients -> single energy value
- direct minimication via. variational principle: tricky part is orthonomality -> QR decomposition implemented in quantum chemistry library DQC -> more robust method, can be applied to large systems

=== methods
- Roothaan equation solved iteratively since Fock matrix is function of C (SCF), computational cost grows in order of n^3 (n depends on C which depends on the number of basis functions) -> SCF may also oscillate
- instead, directly minimize HF energy under orthonomality condition by adjusting coefficents C
  - minimize E(C) while keeping C^T SC = I true
- direct minimization curvilinear search using cayley transformation: minimizes objective function along descent path under the constraint
- augmented langrangian method: solve general constrained optmization problems
  - minimize E(C) subject to h(C) = 0
iteratively solves by reducing to unconstrained optmization problem
- AD of HF energy:
  - input are LCAO coefficents, output is energy value, no calculation of eigenvalues so reverse-mode can be used
  - AD calculate derivative of a function algorithmically based on chain rule
  - symbolic differentiation which generates explicit form of the derivative of a function
- numerical differentian:  estimates value of derivative from fxn values in different points

=== discussion
- good initial guess required to make this approach work (why)
- conjugate gradient vs. gradient
  - combination of conjugate and AD for the future
