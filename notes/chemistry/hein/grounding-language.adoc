---
title: "Grounding"
date: 2024-05-02
---

== Do As I Can, Not As I Say: Grounding Language in Robotic Affordances
"With prompt engineering, a LLM may be capable of splitting the high-level instruction into sub-tasks, but it cannot do so without the context of what the robot is capable of given its abilities and the current state of the robot and the environment."

* robot has atomic behaviours
* system recieves user-provided natural language instruction and set of skills
* probability given skill makes progress towards completing instruction
* constraints

"Prompt engineering provides examples in the context text (“prompt”) for the model that specify the task and the response structure which the model will emulate;"

* value function scores => should output this score
* iteratively append skills that increase value function
* assume optimal set of skills is currently static

"The key idea of SayCan is to ground large language models through value functions – affordance functions that capture the log likelihood that a particular skill will be able to succeed in the current state."

* language conditioned robotic control policies: instantiate robot with set of skills with policy, value function, short description

== Learning How to Ground a Plan – Partial Grounding in Classical Planning

== Grounding natural language instructions to semantic goal representations for abstraction and generalization
* grounding: mapping natural language -> robot behaviour
* choice of representation used to capture objective specified by input command
* markov decision processes
* abstractions in language map to subgoals -> decompose generic, abstract commands into modular subgoals -> robot will be more robsut
* language grounding model -> identify linguistic abstraction -> hierarchical planning -> efficient robot excecution
* abstraction -> key to efficiency
* mapping to fixed sequences of robot actions -> unreliable in changing/stochastic environments
* decouple problem -> statistical language model to map language and robot goals, reward functions as Markov Decision Process (MDP) -> arbitrary planner solves MDP
* novel approaches:
** tackle varying granularity of natural language by mapping to reward functions at different levels of abstraction
** issue of generalization: multistep inference process
* decompose representation and infer constiteunt elements

=== related work
* SHRDLU: handwritten rules, first attempt at agent that can ground language into robot actions
* language -> intermediate -> agent behaviour
** lambda calculus, reward functions, constraints for agent to satisfy in the environment
** learn new grammar rules during course of learning semantic parser, requires a grammar
* goal reward function as conjunction of propostional logic functions
* this work: inference over reward function templates, lifted reward functions -> specify task while learning environment-specific variables of task undefined
** environment binding constraints specify the constraints an object must have to be bound
** output of grounding model never tired to any particular instantiation of environment
** given lifted reward function/environment constraint -> subsequent model can infer environment specific variables -> pass to a planner
* searching an entire search space takes very long -> decompose planning problem into subtasks -> temporal abstraction -> macro actions -> subgoals via fixed sequence of actions or policy with fixed initial/terminal states
* bottom up planning vs. top down (322)L
** bottom up: reward for each action taken backed up through hierarchy of options
** top down (AMDP): determine how good a subgoal is before planning to achieve subgoal
* natural language as a goal state specifiction + action specifiction
** humans mix goal-based commands and action-oriented commands
* deep neural networks (LLM??) -> language grounding
** word embeddings and state of the art RNN

=== background
* markoc decision process:
** five tuple (S, A, T, R, y) set of states, set of actions, transition probability distribution over all possible next states given current state and executed action, R numerical reward earned for particular transition, y is effect time horizon
* object-oriented markov decision process
** to model robot's environment and actions
** builds upon an MDP by adding sets of object classes and propositional functions
** predicates as reward functions -> sufficent semantic representation for grounding language
** map from natural language to propositional reward functions -> correctly encapsulate behavoiur indicated by the input command -> fully specified MDP that can be solved with planning algorithm
** environment specific grounding module -> consumes lifted reward functions and low level binding to specific instances

== semantic goal representation
** break natural langauge into task inference and task execution
** given a language command, find the best r to maximize probability, where r is a lifted propositional function

=== abstraction in language
* high level tasks require long action sequences
* each level of hierarchy requires own set of reward functions
* given a natural language command, find corresponding level of abstracttion and lifted reward function that maximizes joint probability
* language grounding model: infer callable unit, and infer constituent binding arguments -> given natural command, find callable unit u and binding arguments a that maximuze joint probablity

== language grounding models
* use of DRAGGN?

=== grounding module
* "For instance, Artzi and Zettle- moyer (2013) present a model for executing lambda-calculus expressions generated by a combinatory categorical grammar (CCG) semantic parser, which grounds ambiguous predicates and nested arguments."
