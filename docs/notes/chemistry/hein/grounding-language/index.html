<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="x-ua-compatible" content="ie=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Lucy H | Grounding</title>
  <link rel="stylesheet" href="../../../../css/main.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/styles/default.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/highlight.min.js"></script>
  <!-- and it's easy to individually load additional languages -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/languages/scheme.min.js"></script>
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/styles/base16/onedark.min.css" />
  <script>
    hljs.highlightAll();
  </script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

  <link rel="preconnect" href="https://www.googletagmanager.com" />
  <link rel="preconnect" href="https://www.google-analytics.com" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-KDGPVMHC9Q"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());

    gtag("config", "G-KDGPVMHC9Q");
  </script>

  <!-- basic favicon -->
  <link rel="icon" href="../../../../images/android-chrome-384x384.png" />
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../../../../images/android-chrome-384x384.png" />
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../../../../images/android-chrome-384x384.png" />
  <link rel="apple-touch-icon-precomposed" href="../../../../images/android-chrome-384x384.png" />
</head>

  <body>
    <main role="main">
      <div class="flex-row-col">
        <div>
          <h1>Grounding</h1>
          <div>
  
  <div class="header">
    Posted on May  2, 2024
    
  </div>
  <div class="tags">
    
  </div>
  <div class="content">
    <div class="sect1">
<h2 id="_do_as_i_can_not_as_i_say_grounding_language_in_robotic_affordances">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</h2>
<div class="sectionbody">
<div class="paragraph">
<p>"With prompt engineering, a LLM may be capable of splitting the high-level instruction into sub-tasks, but it cannot do so without the context of what the robot is capable of given its abilities and the current state of the robot and the environment."</p>
</div>
<div class="ulist">
<ul>
<li>
<p>robot has atomic behaviours</p>
</li>
<li>
<p>system recieves user-provided natural language instruction and set of skills</p>
</li>
<li>
<p>probability given skill makes progress towards completing instruction</p>
</li>
<li>
<p>constraints</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>"Prompt engineering provides examples in the context text (“prompt”) for the model that specify the task and the response structure which the model will emulate;"</p>
</div>
<div class="ulist">
<ul>
<li>
<p>value function scores ⇒ should output this score</p>
</li>
<li>
<p>iteratively append skills that increase value function</p>
</li>
<li>
<p>assume optimal set of skills is currently static</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>"The key idea of SayCan is to ground large language models through value functions – affordance functions that capture the log likelihood that a particular skill will be able to succeed in the current state."</p>
</div>
<div class="ulist">
<ul>
<li>
<p>language conditioned robotic control policies: instantiate robot with set of skills with policy, value function, short description</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_learning_how_to_ground_a_plan_partial_grounding_in_classical_planning">Learning How to Ground a Plan – Partial Grounding in Classical Planning</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_grounding_natural_language_instructions_to_semantic_goal_representations_for_abstraction_and_generalization">Grounding natural language instructions to semantic goal representations for abstraction and generalization</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>grounding: mapping natural language → robot behaviour</p>
</li>
<li>
<p>choice of representation used to capture objective specified by input command</p>
</li>
<li>
<p>markov decision processes</p>
</li>
<li>
<p>abstractions in language map to subgoals → decompose generic, abstract commands into modular subgoals → robot will be more robsut</p>
</li>
<li>
<p>language grounding model → identify linguistic abstraction → hierarchical planning → efficient robot excecution</p>
</li>
<li>
<p>abstraction → key to efficiency</p>
</li>
<li>
<p>mapping to fixed sequences of robot actions → unreliable in changing/stochastic environments</p>
</li>
<li>
<p>decouple problem → statistical language model to map language and robot goals, reward functions as Markov Decision Process (MDP) → arbitrary planner solves MDP</p>
</li>
<li>
<p>novel approaches:</p>
<div class="ulist">
<ul>
<li>
<p>tackle varying granularity of natural language by mapping to reward functions at different levels of abstraction</p>
</li>
<li>
<p>issue of generalization: multistep inference process</p>
</li>
</ul>
</div>
</li>
<li>
<p>decompose representation and infer constiteunt elements</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_related_work">related work</h3>
<div class="ulist">
<ul>
<li>
<p>SHRDLU: handwritten rules, first attempt at agent that can ground language into robot actions</p>
</li>
<li>
<p>language → intermediate → agent behaviour</p>
<div class="ulist">
<ul>
<li>
<p>lambda calculus, reward functions, constraints for agent to satisfy in the environment</p>
</li>
<li>
<p>learn new grammar rules during course of learning semantic parser, requires a grammar</p>
</li>
</ul>
</div>
</li>
<li>
<p>goal reward function as conjunction of propostional logic functions</p>
</li>
<li>
<p>this work: inference over reward function templates, lifted reward functions → specify task while learning environment-specific variables of task undefined</p>
<div class="ulist">
<ul>
<li>
<p>environment binding constraints specify the constraints an object must have to be bound</p>
</li>
<li>
<p>output of grounding model never tired to any particular instantiation of environment</p>
</li>
<li>
<p>given lifted reward function/environment constraint → subsequent model can infer environment specific variables → pass to a planner</p>
</li>
</ul>
</div>
</li>
<li>
<p>searching an entire search space takes very long → decompose planning problem into subtasks → temporal abstraction → macro actions → subgoals via fixed sequence of actions or policy with fixed initial/terminal states</p>
</li>
<li>
<p>bottom up planning vs. top down (322)L</p>
<div class="ulist">
<ul>
<li>
<p>bottom up: reward for each action taken backed up through hierarchy of options</p>
</li>
<li>
<p>top down (AMDP): determine how good a subgoal is before planning to achieve subgoal</p>
</li>
</ul>
</div>
</li>
<li>
<p>natural language as a goal state specifiction + action specifiction</p>
<div class="ulist">
<ul>
<li>
<p>humans mix goal-based commands and action-oriented commands</p>
</li>
</ul>
</div>
</li>
<li>
<p>deep neural networks (LLM??) → language grounding</p>
<div class="ulist">
<ul>
<li>
<p>word embeddings and state of the art RNN</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_background">background</h3>
<div class="ulist">
<ul>
<li>
<p>markoc decision process:</p>
<div class="ulist">
<ul>
<li>
<p>five tuple (S, A, T, R, y) set of states, set of actions, transition probability distribution over all possible next states given current state and executed action, R numerical reward earned for particular transition, y is effect time horizon</p>
</li>
</ul>
</div>
</li>
<li>
<p>object-oriented markov decision process</p>
<div class="ulist">
<ul>
<li>
<p>to model robot’s environment and actions</p>
</li>
<li>
<p>builds upon an MDP by adding sets of object classes and propositional functions</p>
</li>
<li>
<p>predicates as reward functions → sufficent semantic representation for grounding language</p>
</li>
<li>
<p>map from natural language to propositional reward functions → correctly encapsulate behavoiur indicated by the input command → fully specified MDP that can be solved with planning algorithm</p>
</li>
<li>
<p>environment specific grounding module → consumes lifted reward functions and low level binding to specific instances</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_semantic_goal_representation">semantic goal representation</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>break natural langauge into task inference and task execution</p>
</li>
<li>
<p>given a language command, find the best r to maximize probability, where r is a lifted propositional function</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_abstraction_in_language">abstraction in language</h3>
<div class="ulist">
<ul>
<li>
<p>high level tasks require long action sequences</p>
</li>
<li>
<p>each level of hierarchy requires own set of reward functions</p>
</li>
<li>
<p>given a natural language command, find corresponding level of abstracttion and lifted reward function that maximizes joint probability</p>
</li>
<li>
<p>language grounding model: infer callable unit, and infer constituent binding arguments → given natural command, find callable unit u and binding arguments a that maximuze joint probablity</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_language_grounding_models">language grounding models</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>use of DRAGGN?</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_grounding_module">grounding module</h3>
<div class="ulist">
<ul>
<li>
<p>"For instance, Artzi and Zettle- moyer (2013) present a model for executing lambda-calculus expressions generated by a combinatory categorical grammar (CCG) semantic parser, which grounds ambiguous predicates and nested arguments."</p>
</li>
</ul>
</div>
</div>
</div>
</div>

  </div>
  <div class="flex-row link-no-style">
    
    
  </div>
</div>


        </div>
        <div class="sidebar">
  <a href="../../../../"><h2>Lucy Hao</h2></a>
  <a href="../../../../archive"><h3>Thoughts</h3></a>
  <a href="../../../../notes"><h3>Notes</h3></a>
  <a href="../../../../books"><h3>Books</h3></a>
  <a href="../../../../courses"><h3>Courses</h3></a>
  <a href="../../../../cv/cv.pdf"><h3>CV/Resume</h3></a>
</div>

      </div>
    </main>

    <footer>
      <center>
        <p>
          <a href="../../../../archive">archive</a>
          <a href="https://github.com/lhao03" target="_blank">github</a>
          <a href="https://www.linkedin.com/in/lucy-hao/" target="_blank">linkedin</a>
        </p>
      </center>
      Site powered by cats, 🍫 and
      <a href="http://jaspervdj.be/hakyll">Hakyll</a>
    </footer>
  </body>
</html>
