---
title: "Paper: Fairness and Abstraction in Sociotechnical Systems"
tags: fairness, paper
date: 2022-05-09
---

Andrew D. Selbst, danah boyd, Sorelle A. Friedler, Suresh
Venkatasubramanian, and Janet Vertesi. 2019. Fairness and Abstraction in
Sociotechnical Systems. In FAT* ’19: Conference on Fairness,
Accountability, and Transparency (FAT* ’19), January 29–31, 2019,
Atlanta, GA, USA. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3287560.3287598

* computer science concepts like abstraction and modular design => used
to define notions of fairness
** these concepts render technical interventions ineffective, inaccurate
and sometimes misguide when they enter the societal context

== intro

* fairness aware ML: engineer more fairer and more just ML algorithms
and models by using fairness itself as property of the black box
* by attracting away social context that these systems are to be
deployed, fair-ML researchers miss the broader context including
information to create fairer outcomes or to even understand fairness
** technical systems are subsystems
** fairness and justice are properties of social and legal systems, not
properties of technical systems
** to treat fairness and justice as terms that have meaningful
application to technology separate from social context is to make
category error, or abstraction error
* Science and Technology Studies:
** systems that consist of combination of technical and social
components => sociotechnical systems
* shift away from solutions-oriented approach to a process-oriented
approach
* 1990s: culture of and modes of knowledge production in computer
science thwarted the social goals the field was trying to achieve

== abstraction traps

=== the framing trap

* algorithmic frame: choosing representations of data and labeling of
outcomes => efficacy of algorithm is evaluated as properties of the
output as related to the input
** this abstraction is taken as given and is rarely, if ever
interrogated for validity
** abstraction choices occur implicitly, as accidents of opportunities
and access to data
** fairness cannot be defined in this frame
* data frame: algorithm, and inputs and outputs
** input and output (was part of interface) can now be interrogated
directly => can start thinking about fairness
* sociotechnical frame: ML model is part of sociotechnical system and
that other parts of the system need to be modeled
** the person using the outcome of the ML model can also determine the
model’s fairness => does that person ignore the outcome, or always
follow it, or only sometimes follow it
** failure to incorporate the person’s reaction into the ML model cannot
provide end-to-end guarantee this frame requires

=== portability trap

* repurposing algorithmic solutions designed for one social context to
another
* computer science concept: portability
** suggests design will first aim to create tools independent of social
context
* fair ML:
** fix a definition of fairness as portable module and seek a to
optimize a cost function
** building a "fair wrapper" around the classifier to make resulting
outcomes fair
** both suggestions make certain assumptions
** if social context is modeled well enough and other traps are avoided:
designer has created system not portable between social context

=== formalism trap

* no way to arbitrate between conflicting definitions using purely
mathematical means
** social context must dictate the choice
* maybe no definition is valid to describe fairness
* procedural: law is procedural and fair-ML is outcome-based
* contextuality: we make distinctions all the time, but only cultural
context can determine when basis for discrimination is morally wrong
* contestability: values change > To set them in stone—or in code
[52]—is to pick sides, and to do so without transparent process violates
democratic ideals.

=== ripple effect trap

* failure to understand how insertion of technology into existing social
system changes behaviours and embedded values of pre-existing system
* new tools offer possibility of unconsciously privileging quantifiable
metrics

=== solutionism trap

* law of the instrument: if you have a hammer, everything looks like a
nail
* two broad situations in which starting with technology is wrong
approach or where modeling the situation will not work no matter how
many approximations one makes
** definitions are politically contested or shifting
*** modeling requires pinning down definitions and "code calcifies"
** modeling required would so be so complex as to be computationally
intractable

____
When there is not enough information to understand everything that is
important to a context, approximations are as likely to make things
worse as better.
____

== a sociotechnical perspective

____
Chief among these insights is that the social must be considered
alongside the technical in any design enterprise In reality, technology
always involves social actors.
____

=== STS lens on framing trap

* framing trap is result of only choosing certain technical parts of the
system to model and manage => consider both human activities and machine
one at same time => heterogeneous engineering

____
think simultaneously about what different technical parts of the
apparatus will do, and what the humans that operate, live alongside, and
otherwise contribute to them will do as well.
____

* does not mean we can engineer human decisions, but to draw boundaries
of abstraction to include people and social systems as well

____
Because such systems are inherently fragile and complex, ignoring
certain elements of the network or assuming that they are too unruly or
unpredictable to incorporate undermines the ability of the system to
operate as intended
____

____
As Law puts it, "we must be ready to handle heterogeneity in all its
complexity, rather than adding the social as an explanatory
afterthought."
____

=== STS lens on portability problem

____
Akrich realized that the user "scripts" that dictate how technologies
are supposed to be used only work if all the social and technical
elements of a network are assembled properly. A technology may be
designed with many use cases in mind, but in each case, a designer or
computer scientist hopes to embed certain "scripts" for action into
their product. But the theory of scripts shows that such outcomes will
always be disrupted as soon as the code, device, or software moves to a
different context. At the very least, the code will be taken up in a new
context that shifts the outcome of the system altogether to one that may
or may not be fair.
____

=== STS lens on formalism trap

____
The key elements of the SCOT framework are a period of interpretive
flexibility experienced by relevant social groups, followed by
stabilization, and eventually closure.
____

____
Our choices prioritize certain views over others, exerting power in ways
that must be accounted for. We may privilege the needs of people in our
community—technical practitioners aiming to have precise modules of
portable code or technical academics who need to publish innovative
algorithms over those impacted by the use of fair-ML algorithms.
____

____
In each case, the technologies that "won" did so not because they were
technically superior to their competition, or solved actual users’
problems, or even because their uptake was subject to the free
market—but because of powerful companies or actors with vested interests
in their development. Closure is not always achieved when the best
solution is found; it is typically a byproduct of other social
mechanisms.
____

____
More common is rhetorical closure, which occurs when the relevant social
groups describe the problem as solved, and move on. In some cases, one
design is deemed to achieve this goal, while other functional measures
are said to not matter if this goal is achieved
____

____
In other cases, individuals redefine the problem such that the solution
they already have at hand, or can easily create, becomes the solution to
a problem (i.e. if the algorithm runs the fastest, does it matter if it
is only passably fair?). Pinch and Bijker call this closure by
redefinition of the problem.
____

=== STS lens on ripple effect trap

____
existing groups use the occasion of this new technology to reinforce or
argue for power and position. Computer scientist Rob Kling calls this
process reinforcement politics [44]. In other cases, new technologies
become opportunities to argue for more power in an organizational
context. Finally, the heterogeneous engineer must be aware that once a
technology is part of the social context, new relevant social groups can
arise and radically reinterpret it, return it to a state of interpretive
flexibility, and suggest new mechanisms for closure. In this way,
technologies that were first developed to produce fairness can be
torqued to achieve other aims, even nefarious ones
____

=== STS lens on solutionism trap

____
Computer science programs do not typically incentivize the social
science research necessary to ensure robust system use in the world—or
even to fulfill the Hippocratic oath’s equivalent in engineering to
"first, do no harm."
____
