---
title: "Paper: Efficiently Enabling Block Semantics and Data Updates in DNA Storage"
date: 2023-12-28
tags: synbio, paper
---

https://arxiv.org/abs/2212.13447[Source]

:toc:

== abstract
* storage space into fixed-sized units
* pair of random access PCR primers of length 20 define an independent storage partition, which is managed indepedently of other partitions
* transform internal addressing scheme of a partition into an equivalent PCR-compatible -> run PCR with primers that can be variably elongated to include desired part of internal address
** retrieve a specific block with high accuracy
** 140x reduction in sequencing cost and latency

== introduction
* similarity search
* runtimes for automating wetlab protocols
* provide random access at nearly constant latency via PCR
* current state of the art: key-valye object stores
** pair of primers define kay and arbitrarly sized value stored in molecules tagged with same primers
* large objects stored across many molecules, all of which are retrieved with sufficient uniformity
** internal address: part of molecule that uniquely identify and re-establish order
** object than spans N molecules requires log_4 N bases for indexing
* primers:
** lots of unique combinations but all aren't appropriate
** must be balanced: GC content (50%)
** all primers used in same DNA sample must be significantly different from each other in Hamming distance to avoid amplication of unwanted data
*** minimum pairwise distance (a problem) only around 1000-3000 primers can fit the constraints
* can only chemically distinguish 1000 different objects
* with 1TB of data, the smallest unit of access is 1GB, so we always have to sequence this much DNA, no matter how small the unit we want is
* using longer primers doesn't work: compatible primers scale linearly
* can't build a functional data storage because of:
** allowing arbitrary size, can't easily get small objects
*** high Hamming distance
*** many cycles of PCR
*** object-based design results in flat architechture
**** no logical order/distance metric between objects
*** limiting sequential access to a single object -> random access

paper's proposal: block storage semantics
- each block independently read/written to
- group of consecutive blocks can be efficieetnly retriveed
- numer of mutually compatible primer pairs is limited, internal address space avalible to any pair of primers is virutally unlimited

contributions
- flexibility and constraints of internal address space for pair of primers -> implement block storage sementics
- PCR-compatible indexes to enable random access with primers than can be enlongated
- organize partitions similar to version control
- wetlab evaluation with state-of-the-art DNA storage architecture

== background

=== 2.1 dna storage basics
- dna molecule 300 bases long can store 75 bytes
- have to break down files into shorter molecules, and add an internal address or index

==== 2.1.1 encoding
* using a coding scheme (constrained coding) that
** prevent occurrence of homopolyers
** balance GC content
* unconstrained coding: homopolymers occur with low probability
** significantly increases coding density with practical ranges of error rates
** used in this paper
** handle error types with outer Reed-Solomon ECC codes
* to encode internal address within every molecule
** use more complex constrained coding scheme

image::/images/block/fig1.png[]

* once file split into pieces and encoded into DNA, pair of primers added to end and beginning of each string
** chemical tag that logically group related molecules together, allowing for random access

==== 2.1.2 data retrieval
* PCR used to selectively amplify molecules containing file peices
** molecules isolated using primers
** sequencing produces many DNA strings: reads
** errors are manifested in reads
** sequencing coverage/depth: the higher the coverage, the easier to reconstruct from these reads, but higher cost
* obtained reads clustered based on similarity (should be from same molecule) via Levenshtein distance
* consensus finding algorithm to extract original DNA string

==== 2.1.3 decoding and error correction
* using internal address, binary data used to recreate original file
* errors corrected using Reed-Solomon or LDPC codes
* state of the art architecture treats all DNA molecules as columns in matrix
** seperate DNA molecules created as external ECC code, one codeword rerepsent a row -> high information density it creates strong inter-molecular dependencies

==== 2.1.4 PCR
* doubles number of DNA molecules in every cycle
* three phases: denaturation, annealing, extension
* optimal primer length: 20
* can go to 40 and max length 100
* internal addresses can be very similar, not balanced GC content and repeated base

=== existing data update mechanisms
* direct edits of DNA molecules: limited to updates to single molecule, can only be applied when size of data does not change
* current state of the art orgzanizations have data split across many molecules, significant intermolecular data dependencies
* instead of storing data as nucleotides, store as nicks: rewritable
** but cannot do PCR, sacrificing random access
** storage density is 50-fold lower

== 3 managing internal address space
* pair of primers of length 20
* length of DNA strands: 150
* maximum storage capacity achieved when entire portion of NA used for indexing (no actual data stored)
* density highest when onle one molecule, no index needed (only for tiny objects)
* longer primers reduct information density significantly, loss diminishes linearly with longer strand length

== 3.1 parition architecture
* address space AAA...AAA to TTT...TTT with index length L represented as prefix tree
** index of length L covers addresses from AAA...AAA to TTT...TTT: 1D array of 4^L fixed capacity storage units (payload of one molecule)
**  any contigous byte-range can be statically mapped to contigous index-range and vice versa
* contiguous index-range be can precisely described with few prefixes: AAA to AGT -> AA, AC, AG and AT
* leaf is DNA strand
* maximum information density achieved with all DNA strands have same index length and all indexes present
* any contiguous range of bytes within parition could be retrived quite precisely with single PCR if primers were extended to include part of the index
* future work: to improve efficiency of sequential accesses: set of files could be mapped onto the partition in a manner that tries to optimally align the files to nodes in prefix tree

== 3.2 concentration constraints
* to manage cost of sequencing: every strand in DNA storage should be represented in equal concentrations
** highly concentrated strands will be sequenced at higher coverage while other strands need more sequencing to be represented, wasting resources
** for random black access to work: desired sequences need to be amplified much higher because indexes are similar
** PCR may overwrite their index to desired index, resulting in exponential amplification -> mispriming
*** multiple dna strands, can't use consensus to select which one
** to ensure dominance of desired strands: ensure closet targets in pool are not present in higher concentration
* all nodes at same level of index tree map to similar number of DNA strands and in similar concentrations
** make sure data updates do not compromise this balance

== 4 random block access

== 4.1 general approach
* indexes AAA...AAA to TTT...TTT are not PCR-compatible

image::/images/block/primer.png[]

* prior work: follow maximum information density design -> no control over their structure
* use less dense encoding of indexes, minor loss in information density
* added sparsity provide protection against errors in index

== 4.2 requirements for elongated primers
* every pair of main primer: high mutual distance so can extract target partition regardless of its size and concentration relative to other paritions
** two similar primers: P_a and P_b
*** target data is dominant because of uniform concentration of data within same level of index hierarchy
*** ensuring high distance between indexes, is not as important as it is for main primers
* GC content: primers have slightly more restrictions
** primers are not fixed length: add sparsity to indexes so GC content is uniform with every elongation

== 4.3 PCR-Navigable Index Tree
* prefix tree is randomized (the order of edges): to prevent incomplete, unbalanced, degenerate trees
* sparse out addresses: add extra letter between every two adjacent edges
** perfectly balance the GC content (maximize Hamming distance between sibling nodes)
*** guatantees near perfect GC content and disable indexes of more than 2
* 10-based long internal address: 3% information density loss
** added flexibility reduces sequencing costs

== 4.4 Index Tree Management
* different seeds for different partitions to ensure vastly different trees to avoid unwanted molecular interactions

== 5 Data Updates in DNA Storage
* low-latency enzymatic synthesis

== 5.1 naive
* throw away old data, update software to just use new primers
* problems:
** recreate parition (expensive, arbitrary amount of information)
** wasting primres

== 5.2 versioning
* updates logged as ordered series of incremental patches
* minimal set of DNA molecules at same concentration
* application of updates done in software, not in-situ chemically
* how should updates be tagged and their primers?
* how to retrieve updated DNA and how to mix original DNA with updated DNA

== 5.3 placement of updates in address space
* updates in their own partition with dedicated set of primers
* have to potentially read all updates, none of which apply to data (becaues data might have been updated)
* updates embedded into address space of each primer pair -> reading updated data requires single PCR
** but have to read all data under same primer pair -> lots of data
* final: data and updates interwoven
** allocate blocks for updates, some unused, overflow data is given a pointer
*** object with prefix ACGT: original object at ACGTA, first update: ACGTC, second update ACGTG
**** link between data and updates, no bookkeeping
*** PCR uses primers with ACGT, software understands how to patch everything together

== 5.4 structure and semantics of updates
* ensure concentrations of original DNA and updates are similar as possible
* application of updates delegated to end-user or upper-application

== 5.5 physically mixing data and updates
* directly impacts cost of sequencing

== 6 methodology (skipped for now)

== 7 results

== 7.1 baseline random access
* sequencing of 99.66% unwanted data

== 7.2 random block access
* mispriming is an issue with similar primers
* most errors caused by blocks amplified through mispriming

== 7.3 sequencing cost reduction
* ability to retrieve individual data blocks
* sequencing cost is proportional to size of sequencing cost

== 7.4 sequencing latency reduction
* ability to select given block, but latency depends on partition size and sequencing technology
** block based: retrivign single block reduces number of runs needed
* NGS: sequencing output only avalible at end of run
* nanopore: output size dependent, continuosly produced and analyzed in real time
* post-sequencing/data movement and software decoding time also reduced (not a bottleneck so doesn't really matter)

== 7.5 cost of creating and retrieving updates
* naive system: new updated copy of entire partition and assigns new primer
* our system: precise access retrieves data and updates for specific block

== 7.6 mixing data and updates
* possible to keep concentrations of data and updates similar

== 7.7 scalabilty and limitations !!
=== 7.7.1 block count
* extend both primers instead of one
** two-sided extension by 10 characters -> 1024^2 addressable blocks, same order of magnitude as modern SSDs

=== 7.7.2 block size
* mispriming depends on number of blocks and structure and sparsity of their indexes
* no limits on block size

=== 7.73 partition count
* prevent mispriming: two PCR cycles
** first main primers
** second elongated primers
* zipfan distrbution: many blocks never accessed, few accessed very frequently

== 8 decoding procedure
* extract substring between elongated forward primer and reverse primer (payload)
* cluster payloads as per Rashtchian
* in descending order of cluster size: trace reconstruction using double sided BMA algorithm

== 8.1 handing of mispriming during pcr
* incorrectly amplified strands: indexes that were very close to indexes of target block in edit distance (2-3), rather than hamming
* to an extent, can be corrected through error correction codes

== 9 related work
* nested PCRs
** don't support multiplex-PCR (this does)
* nested PCR is only two levels deep, this is 6 levels deep

== conclusion
* slow storage media (DNA) so should do update applications in software
